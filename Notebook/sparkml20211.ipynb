{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Project 20211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spark-nlp==3.0.1\n",
      "  Downloading spark_nlp-3.0.1-py2.py3-none-any.whl (146 kB)\n",
      "     |████████████████████████████████| 146 kB 965 kB/s            \n",
      "\u001b[?25hCollecting numpy==1.21.4\n",
      "  Downloading numpy-1.21.4-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "     |████████████████████████████████| 15.7 MB 4.6 MB/s            \n",
      "\u001b[?25hCollecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.3.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "     |████████████████████████████████| 11.5 MB 75.8 MB/s            \n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.4 MB)\n",
      "     |████████████████████████████████| 26.4 MB 78.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas->-r requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas->-r requirements.txt (line 5)) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 5)) (1.16.0)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "     |████████████████████████████████| 306 kB 91.9 MB/s            \n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Collecting scipy>=1.1.0\n",
      "  Downloading scipy-1.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.8 MB)\n",
      "     |████████████████████████████████| 39.8 MB 130 kB/s             \n",
      "\u001b[?25hBuilding wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1315 sha256=cf53589983cd71927994d3ec7117c6863973f50a9bc81f5ff2c3b674a53c2c71\n",
      "  Stored in directory: /root/.cache/pip/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: numpy, threadpoolctl, scipy, joblib, scikit-learn, spark-nlp, sklearn, pandas\n",
      "Successfully installed joblib-1.1.0 numpy-1.21.4 pandas-1.3.5 scikit-learn-1.0.2 scipy-1.7.3 sklearn-0.0 spark-nlp-3.0.1 threadpoolctl-3.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a9b3e03b-1d16-40cb-b5f6-287a2f5120d9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;3.0.1 in central\n",
      "\tfound com.typesafe#config;1.3.0 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.5.3 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.603 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound org.json4s#json4s-ext_2.12;3.5.3 in central\n",
      "\tfound joda-time#joda-time;2.9.5 in central\n",
      "\tfound org.joda#joda-convert;1.8.1 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.2.2 in central\n",
      "\tfound net.sf.trove4j#trove4j;3.0.3 in central\n",
      ":: resolution report :: resolve 396ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.603 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;3.0.1 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.2.2 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.3.0 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjoda-time#joda-time;2.9.5 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\tnet.sf.trove4j#trove4j;3.0.3 from central in [default]\n",
      "\torg.joda#joda-convert;1.8.1 from central in [default]\n",
      "\torg.json4s#json4s-ext_2.12;3.5.3 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.5.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   21  |   0   |   0   |   0   ||   21  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a9b3e03b-1d16-40cb-b5f6-287a2f5120d9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 21 already retrieved (0kB/11ms)\n",
      "22/01/02 16:18:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://7a672c5fc605:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7faf78c51460>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        config(\"spark.driver.memory\",\"16G\").\\\n",
    "        config(\"spark.driver.maxResultSize\", \"0\").\\\n",
    "        config(\"spark.kryoserializer.buffer.max\", \"2000M\").\\\n",
    "        config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.1\").\\\n",
    "        getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# dataset = spark.read \\\n",
    "#       .option(\"header\", True) \\\n",
    "#       .json(\"data/Prime_Pantry.json\")\n",
    "dataset = spark.read \\\n",
    "      .option(\"header\", True) \\\n",
    "      .option(\"inferSchema\", True) \\\n",
    "      .option('quote', '\"') \\\n",
    "      .option('escape', '\"') \\\n",
    "      .csv(\"hdfs://namenode:9000/data/input/reviews_0.csv\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|overall| count|\n",
      "+-------+------+\n",
      "|      3| 97031|\n",
      "|      5|465478|\n",
      "|      1|107080|\n",
      "|      4|149331|\n",
      "|      2| 64718|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter rows with include positive and negative reviews\n",
    "dataset = dataset.filter(dataset.overall.isin([1, 2, 3, 4, 5]))\n",
    "# count positive and negative reviews\n",
    "dataset.groupBy('overall').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+--------------+----------+------------+--------------------+--------------------+--------------+----+-----+-----+--------+\n",
      "|overall|verified| reviewTime|    reviewerID|      asin|reviewerName|          reviewText|             summary|unixReviewTime|vote|style|image|  target|\n",
      "+-------+--------+-----------+--------------+----------+------------+--------------------+--------------------+--------------+----+-----+-----+--------+\n",
      "|      5|    True|10 20, 2014|A1D4G1SNUZWQOT|7106116521|       Tracy|Exactly what I ne...|perfect replaceme...|    1413763200|null| null| null|positive|\n",
      "|      2|    True|09 28, 2014|A3DDWDH9PX2YX2|7106116521|   Sonja Lau|I agree with the ...|I agree with the ...|    1411862400| 3.0| null| null|negative|\n",
      "|      4|   False|08 25, 2014|A2MWC41EW7XL15|7106116521|    Kathleen|Love these... I a...| My New 'Friends' !!|    1408924800|null| null| null|positive|\n",
      "|      2|    True|08 24, 2014|A2UH2QQ275NV45|7106116521| Jodi Stoner| too tiny an opening|           Two Stars|    1408838400|null| null| null|negative|\n",
      "|      3|   False|07 27, 2014| A89F3LQADZBS5|7106116521|Alexander D.|                Okay|         Three Stars|    1406419200|null| null| null|negative|\n",
      "+-------+--------+-----------+--------------+----------+------------+--------------------+--------------------+--------------+----+-----+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|  target| count|\n",
      "+--------+------+\n",
      "|positive|614809|\n",
      "|negative|268829|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# apply overall > 3.0 to positive reviews, else negative\n",
    "dataset = dataset.withColumn('target', F.when(F.col('overall') > 3.0, 'positive').otherwise('negative'))\n",
    "dataset.show(5)\n",
    "# count positive and negative reviews\n",
    "dataset.groupBy('target').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "| overall|          reviewText|\n",
      "+--------+--------------------+\n",
      "|positive|Im a die hard Dad...|\n",
      "|negative|This movie was a ...|\n",
      "|negative|I just sat in the...|\n",
      "|positive|Just saw this fil...|\n",
      "|negative|I consider myself...|\n",
      "+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset[['overall', 'reviewText']]\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand \n",
    "dataset = dataset.orderBy(rand()) # Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# stop words\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "|          reviewText| overall|               words|            filtered|            features|label|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "|\"Dead Man Walking...|positive|[dead, man, walki...|[dead, man, walki...|(10000,[0,1,2,3,4...|  1.0|\n",
      "|\"Fat Girls\" is am...|negative|[fat, girls, is, ...|[fat, girls, is, ...|(10000,[0,1,2,3,4...|  0.0|\n",
      "|\"Riders of Destin...|negative|[riders, of, dest...|[riders, of, dest...|(10000,[0,1,2,3,5...|  0.0|\n",
      "|\"Valley Girl\" lau...|positive|[valley, girl, la...|[valley, girl, la...|(10000,[0,1,2,3,4...|  1.0|\n",
      "|*Spoilers herein*...|negative|[spoilers, herein...|[spoilers, herein...|(10000,[0,1,2,3,4...|  0.0|\n",
      "|...I saw this on ...|positive|[i, saw, this, on...|[i, saw, this, on...|(10000,[0,1,2,3,4...|  1.0|\n",
      "|<br /><br />In th...|negative|[br, br, in, the,...|[br, br, in, proc...|(10000,[0,1,2,3,5...|  0.0|\n",
      "|<br /><br />This ...|negative|[br, br, this, mo...|[br, br, this, mo...|(10000,[0,2,5,6,9...|  0.0|\n",
      "|A Cinderella stor...|negative|[a, cinderella, s...|[a, cinderella, s...|(10000,[0,1,2,4,5...|  0.0|\n",
      "|A Vow to Cherish ...|positive|[a, vow, to, cher...|[a, vow, to, cher...|(10000,[0,1,2,3,4...|  1.0|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "|          reviewText| overall|               words|            filtered|            features|label|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "|\"Throw Momma From...|positive|[throw, momma, fr...|[throw, momma, fr...|(10000,[0,1,2,3,4...|  1.0|\n",
      "|A Formula For Mur...|positive|[a, formula, for,...|[a, formula, for,...|(10000,[0,1,2,3,4...|  1.0|\n",
      "|A less than redem...|negative|[a, less, than, r...|[a, less, than, r...|(10000,[0,1,2,3,4...|  0.0|\n",
      "|After the return ...|positive|[after, the, retu...|[after, return, o...|(10000,[0,1,2,4,6...|  1.0|\n",
      "|Although others h...|positive|[although, others...|[although, others...|(10000,[0,1,2,3,4...|  1.0|\n",
      "|As I saw the movi...|positive|[as, i, saw, the,...|[as, i, saw, movi...|(10000,[0,2,3,6,8...|  1.0|\n",
      "|Before the Intern...|negative|[before, the, int...|[before, internet...|(10000,[0,1,2,3,4...|  0.0|\n",
      "|Best animated mov...|positive|[best, animated, ...|[best, animated, ...|(10000,[0,1,2,3,4...|  1.0|\n",
      "|Bizarre horror mo...|positive|[bizarre, horror,...|[bizarre, horror,...|(10000,[0,1,2,3,4...|  1.0|\n",
      "|Charles Boyer is ...|positive|[charles, boyer, ...|[charles, boyer, ...|(10000,[0,1,2,3,4...|  1.0|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "trainingData.show(10)\n",
    "testData.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "cvModel = cv.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                    reviewText| overall|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|\"Throw Momma From the Train...|positive|[0.004367934986061326,0.995...|  1.0|       1.0|\n",
      "|A Formula For Murder isn't ...|positive|[0.03301721350734545,0.9669...|  1.0|       1.0|\n",
      "|A less than redemptive hunk...|negative|[0.7608247074504314,0.23917...|  0.0|       0.0|\n",
      "|After the return of \"horror...|positive|[0.4902780675040531,0.50972...|  1.0|       1.0|\n",
      "|Although others have commen...|positive|[0.40823599145099165,0.5917...|  1.0|       1.0|\n",
      "|As I saw the movie I was re...|positive|[0.47973990133809774,0.5202...|  1.0|       1.0|\n",
      "|Before the Internet this mo...|negative|[0.8852763163972478,0.11472...|  0.0|       0.0|\n",
      "|Best animated movie ever ma...|positive|[0.13445387145687182,0.8655...|  1.0|       1.0|\n",
      "|Bizarre horror movie filled...|positive|[0.1695298396595365,0.83047...|  1.0|       1.0|\n",
      "|Charles Boyer is supposed t...|positive|[0.18179289608805008,0.8182...|  1.0|       1.0|\n",
      "|Creature Unknown is the rig...|negative|[0.9024108658216098,0.09758...|  0.0|       0.0|\n",
      "|Cummings is falsely accused...|positive|[0.2748589012527674,0.72514...|  1.0|       1.0|\n",
      "|For the most part, \"Michael...|negative|[0.7524123364333132,0.24758...|  0.0|       0.0|\n",
      "|Good actors, good director,...|negative|[0.6956440174136079,0.30435...|  0.0|       0.0|\n",
      "|Has there ever been a movie...|positive|[0.21757560565018672,0.7824...|  1.0|       1.0|\n",
      "|I am amazed that movies lik...|positive|[0.26151026391352655,0.7384...|  1.0|       1.0|\n",
      "|I consider myself to have a...|negative|[0.7179923414756959,0.28200...|  0.0|       0.0|\n",
      "|I forsee many students now ...|positive|[0.7504916690351584,0.24950...|  1.0|       0.0|\n",
      "|I just recently stumbled up...|positive|[0.14849116912403415,0.8515...|  1.0|       1.0|\n",
      "|I remember seeing this film...|positive|[0.39972917953927256,0.6002...|  1.0|       1.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = cvModel.transform(testData)\n",
    "predictions = predictions.select(\"reviewText\",\"overall\",\"probability\",\"label\",\"prediction\")\n",
    "predictions.show(n = 20, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8644098421108525"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.91      0.90      4931\n",
      "         1.0       0.91      0.89      0.90      5101\n",
      "\n",
      "    accuracy                           0.90     10032\n",
      "   macro avg       0.90      0.90      0.90     10032\n",
      "weighted avg       0.90      0.90      0.90     10032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We are going to use sklearn to evalute the results on test dataset\n",
    "from sklearn.metrics import classification_report\n",
    "print (classification_report(predictions['prediction'], predictions['label']))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15d21c04e104a7fd0ffe6fbe0eb4d71583944ad5de2575e56bdd4886a5c334cc"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
